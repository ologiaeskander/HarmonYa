# -*- coding: utf-8 -*-
"""Annoy Cosine similarity.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yooVBCIX0AaiLGpYZ3eXgTXNPYfkOfa4

1. Imports and Setup
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from collections import defaultdict
from sklearn.preprocessing import StandardScaler
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.neighbors import NearestNeighbors
from google.colab import drive
import os
import warnings
warnings.filterwarnings('ignore')
from IPython import get_ipython
from IPython.display import display

"""Import Annoy"""

try:
    import annoy
except ImportError:
    print("Installing Annoy...")
    !pip install annoy
    import annoy

from annoy import AnnoyIndex

"""2. Data Loading and Preprocessing"""

# Loading and preprocessing function
def load_and_preprocess(data_path):
    df = pd.read_csv(data_path)

    # Select and normalize audio features
    audio_features = [
        'danceability', 'energy', 'key', 'loudness', 'mode',
        'speechiness', 'acousticness', 'instrumentalness',
        'liveness', 'valence', 'tempo', 'duration_ms', 'time_signature'
    ]

    # Clean data
    df = df.dropna(subset=audio_features).copy()
    df = df.reset_index(drop=True)

    # Normalize features
    scaler = StandardScaler()
    features = scaler.fit_transform(df[audio_features])

    # Add track_id if not present
    if 'id' not in df.columns:
        df['id'] = df.index.astype(str)

    return df, features, audio_features

# Mount Google Drive
drive.mount('/content/drive')

# Load data and pass it into the preprocessing function
file_path = '/content/drive/MyDrive/Graduation Project/Dev/tracks_features.csv'
df, features, audio_features = load_and_preprocess(file_path)
print(f"Loaded {len(df)} tracks with {features.shape[1]} features each")

"""3. Build Similarity Model"""

def get_annoy_index(features, index_path='spotify_index.ann'):
    dim = features.shape[1]

    # Load if exists
    if os.path.exists(index_path):
        index = AnnoyIndex(dim, 'angular')
        index.load(index_path)
        return index

    # Otherwise build and save
    index = AnnoyIndex(dim, 'angular')
    for i, vec in enumerate(features):
        index.add_item(i, vec)
    index.build(20)  # 20 trees for good accuracy
    index.save(index_path)

    return index

"""4. Recommendation Functions"""

def recommend_from_playlist(playlist_track_ids, df, annoy_index, n_recommend=20):
    # Convert track IDs to indices
    track_indices = []
    for tid in playlist_track_ids:
        matches = df[df['id'] == str(tid)]
        if not matches.empty:
            track_indices.append(matches.index[0])

    if not track_indices:
        return []

    # Score candidates
    candidate_scores = defaultdict(float)
    playlist_set = set(track_indices)

    for track_idx in track_indices:
        neighbors, dists = annoy_index.get_nns_by_item(
            track_idx, 50, include_distances=True)

        for n_idx, dist in zip(neighbors, dists):
            if n_idx not in playlist_set:
                cosine_sim = 1 - (dist**2)/2
                candidate_scores[n_idx] += cosine_sim

    # Normalize and get top tracks
    top_indices = sorted(candidate_scores.keys(),
                        key=lambda x: -candidate_scores[x])[:n_recommend]
    return [df.iloc[idx]['id'] for idx in top_indices]

"""5. Demo Usage

Simple sanity check
"""

if __name__ == "__main__":
    # Load data
    df, features, audio_features = load_and_preprocess('/content/drive/MyDrive/Graduation Project/Dev/tracks_features.csv')

    # Get index (builds only first time)
    annoy_index = get_annoy_index(features)

    # Test with random playlist
    test_playlist = df.sample(5)['id'].tolist()
    recs = recommend_from_playlist(test_playlist, df, annoy_index)
    print(f"Recommended tracks: {recs[:5]}")  # Show first 5 recs

"""6. Actual test with real user playlist. Ignore this part. The model is weak by design and can't actually produce meaningful results.

from scipy.sparse import load_npz
import random

def evaluate_recommendations(df, annoy_index, sparse_matrix_path, n_tests=10):

    #Evaluate recommendations using real playlist data
    #Args:
        #df: Your tracks dataframe
        #annoy_index: Built Annoy index
        #sparse_matrix_path: Path to playlist_track_sparse_matrix.npz
        #n_tests: Number of playlists to test on
    # Load the sparse matrix
    playlist_track_matrix = load_npz(sparse_matrix_path).tocsr()

    # Track evaluation metrics
    recalls = []

    for _ in range(n_tests):
        # Select a random playlist
        playlist_idx = random.randint(0, playlist_track_matrix.shape[0]-1)

        # Get all tracks in this playlist
        _, track_indices = playlist_track_matrix[playlist_idx].nonzero()

        # Skip very small playlists
        if len(track_indices) < 5:
            continue

        # Split into known (80%) and hidden (20%) tracks
        split_point = int(0.8 * len(track_indices))
        known_indices = track_indices[:split_point]
        hidden_indices = track_indices[split_point:]

        # Get track IDs
        known_tracks = df.iloc[known_indices]['id'].tolist()
        hidden_tracks = df.iloc[hidden_indices]['id'].tolist()

        # Get recommendations
        recommendations = recommend_from_playlist(known_tracks, df, annoy_index, 20)

        # Calculate recall (how many hidden tracks were recommended)
        found = len(set(recommendations) & set(hidden_tracks))
        recall = found / len(hidden_tracks)
        recalls.append(recall)

        print(f"Playlist {playlist_idx}:")
        print(f"  Hidden tracks: {len(hidden_tracks)}")
        print(f"  Found {found} (Recall: {recall:.2f})")

    # Print summary statistics
    print("\nEvaluation Summary:")
    print(f"Average recall@20: {np.mean(recalls):.2f}")
    print(f"Median recall@20: {np.median(recalls):.2f}")
    print(f"Max recall@20: {np.max(recalls):.2f}")
    print(f"Min recall@20: {np.min(recalls):.2f}")

if __name__ == "__main__":
    # Load data
    df, features, audio_features = load_and_preprocess('/content/drive/MyDrive/Graduation Project/Dev/tracks_features.csv')

    # Get index
    annoy_index = get_annoy_index(features)

    # Run both simple test and proper evaluation
    print("=== Simple Test ===")
    test_playlist = df.sample(5)['id'].tolist()
    recs = recommend_from_playlist(test_playlist, df, annoy_index)
    print(f"Recommended tracks: {recs[:5]}")

    print("\n=== Proper Evaluation ===")
    evaluate_recommendations(
        df,
        annoy_index,
        '/content/drive/MyDrive/Graduation Project/Dev/playlist_track_sparse_matrix.npz'
    )

Added to make the code work when combined with the other model
"""

def get_content_recommendations(track_ids, df, annoy_index, top_k=50):
    """Get raw content-based recommendations"""
    return recommend_from_playlist(track_ids, df, annoy_index, n_recommend=top_k)

def load_content_model(index_path, data_path):
    """Load the content-based model and data"""
    df, features, audio_features = load_and_preprocess(data_path)
    annoy_index = get_annoy_index(features, index_path)
    return df, annoy_index

"""Save Model"""

model = annoy_index
model.save('/content/drive/MyDrive/Graduation Project/Dev/Content_Filtering.ann')