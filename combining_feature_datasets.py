# -*- coding: utf-8 -*-
"""Combining Feature datasets.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1utH0yEdSSpQk11mC24N_VMB5G3rlitIu
"""

import numpy as np
import pandas as pd
from scipy.sparse import save_npz
import pickle
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler
from tqdm import tqdm

# Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

def generate_synthetic_features(real_features_path, track_index_to_uri, output_path):
    """
    Generate synthetic features combining real data and synthetic generation
    for tracks missing in the real features dataset.
    """
    # Load real features
    real_features = pd.read_csv(real_features_path)

    # Standardize track_id format if needed
    if 'id' in real_features.columns:
        real_features['track_id'] = real_features['id'].apply(lambda x: f"spotify:track:{x}")
    elif 'track_uri' in real_features.columns:
        real_features['track_id'] = real_features['track_uri']

    # Feature columns to use
    feature_cols = [
        'danceability', 'energy', 'key', 'loudness', 'mode',
        'speechiness', 'acousticness', 'instrumentalness',
        'liveness', 'valence', 'tempo', 'duration_ms'
    ]

    # Create full feature matrix
    num_tracks = len(track_index_to_uri)
    feature_matrix = np.zeros((num_tracks, len(feature_cols)))
    tracks_with_features = set(real_features['track_id'])

    # Fill in real features
    track_uri_to_idx = {uri: idx for idx, uri in track_index_to_uri.items()}
    for _, row in real_features.iterrows():
        if row['track_id'] in track_uri_to_idx:
            idx = track_uri_to_idx[row['track_id']]
            feature_matrix[idx] = row[feature_cols].values

    # Generate synthetic features for missing tracks
    # First impute missing values with mean
    imputer = SimpleImputer(strategy='mean')
    imputed_features = imputer.fit_transform(feature_matrix)

    # Then add some noise to make synthetic values more realistic
    noise = np.random.normal(0, 0.1, imputed_features.shape)
    synthetic_features = imputed_features + noise

    # Clip values to valid ranges for each feature
    # Spotify features are typically between 0-1 for most features
    synthetic_features[:, :2] = np.clip(synthetic_features[:, :2], 0, 1)  # danceability, energy
    synthetic_features[:, 2] = np.clip(synthetic_features[:, 2], 0, 11).astype(int)  # key
    synthetic_features[:, 4] = np.clip(synthetic_features[:, 4], 0, 1).astype(int)  # mode
    synthetic_features[:, 5:9] = np.clip(synthetic_features[:, 5:9], 0, 1)  # speechiness, acousticness, instrumentalness, liveness
    synthetic_features[:, 9] = np.clip(synthetic_features[:, 9], 0, 1)  # valence

    # Normalize all features
    scaler = StandardScaler()
    normalized_features = scaler.fit_transform(synthetic_features)

    # Create DataFrame with all features
    all_features = pd.DataFrame(normalized_features, columns=feature_cols)
    all_features['track_id'] = [track_index_to_uri[i] for i in range(num_tracks)]

    # Save complete features
    all_features.to_csv(output_path, index=False)
    return all_features

def build_complete_feature_dataset(track_index_to_uri, dataset_paths):
    """
    Build a complete feature dataset by combining multiple sources
    Returns: DataFrame with track_id and audio features
    """
    # Load all datasets with progress tracking
    dfs = []
    for path in tqdm(dataset_paths, desc="Loading datasets"):
        df = pd.read_csv(path)

        # Standardize column names (spotify:track: prefix handling)
        if 'id' in df.columns:
            df['track_id'] = df['id'].apply(lambda x: f"spotify:track:{x}")
        elif 'track_uri' in df.columns:
            df['track_id'] = df['track_uri']

        dfs.append(df)

    # Combine with priority to first dataset's values
    combined = pd.concat(dfs).drop_duplicates('track_id', keep='first')

    # Select relevant audio features (standardize across datasets)
    feature_cols = [
        'danceability', 'energy', 'key', 'loudness', 'mode',
        'speechiness', 'acousticness', 'instrumentalness',
        'liveness', 'valence', 'tempo', 'duration_ms'
    ]

    # Filter to only tracks in our mapping
    our_tracks = set(track_index_to_uri.values())
    filtered = combined[combined['track_id'].isin(our_tracks)]

    print(f"Found {len(filtered)}/{len(our_tracks)} tracks ({len(filtered)/len(our_tracks):.1%})")
    return filtered[['track_id'] + feature_cols]

# Load the track index to URI mapping
with open('/content/drive/MyDrive/Graduation Project/Dev/track_index_to_uri.pkl', 'rb') as f:
    track_index_to_uri = pickle.load(f)

# Define dataset paths
dataset_paths = [
    '/content/drive/MyDrive/Graduation Project/Dev/tracks_features.csv',
    '/content/drive/MyDrive/Graduation Project/Dev/SpotifyFeatures.csv'
]

# Build and save combined features
complete_features = build_complete_feature_dataset(track_index_to_uri, dataset_paths)
complete_features.to_csv('/content/drive/MyDrive/Graduation Project/Dev/combined_audio_features.csv', index=False)


print("Dataset combination complete!")

if __name__ == "__main__":
    # Load your existing mappings
    with open('/content/drive/MyDrive/Graduation Project/Dev/track_index_to_uri.pkl', 'rb') as f:
        track_index_to_uri = pickle.load(f)

    # Generate synthetic features
    generate_synthetic_features(
        real_features_path='/content/drive/MyDrive/Graduation Project/Dev/combined_audio_features.csv',
        track_index_to_uri=track_index_to_uri,
        output_path='/content/drive/MyDrive/Graduation Project/Dev/complete_audio_features.csv'
    )