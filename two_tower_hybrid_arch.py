# -*- coding: utf-8 -*-
"""Two Tower Hybrid Arch.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bMIe-FS6wSa907jRN-C8_E8hdg8uO1Tg

# 1. Imports and Setup
"""

import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers, ops
from keras.saving import register_keras_serializable
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import roc_auc_score, f1_score
from typing import List, Dict, Tuple, Optional
import pickle
from pathlib import Path
from collections import defaultdict
from scipy.sparse import load_npz
import json

# Install Annoy if needed
try:
    import annoy
except ImportError:
    print("Installing Annoy...")
    !pip install annoy
    import annoy
from annoy import AnnoyIndex

# Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

# Path configurations
base_path = Path('/content/drive/MyDrive/Graduation Project/Dev')

"""Load the test dataset"""

def load_test_playlists(json_path: Path) -> List[Dict]:
    """Load test playlists from JSON file"""
    with open(json_path, 'r') as f:
        data = json.load(f)
    return data['playlists']

"""# 2. CUSTOM CLASSES

Binary F1 Score (from the Collaborative Filtering Model)
"""

@tf.keras.utils.register_keras_serializable()
class BinaryF1Score(keras.metrics.Metric):
    """Custom F1 Score metric for model evaluation"""
    def __init__(self, name='f1_score', threshold=0.5, **kwargs):
        super().__init__(name=name, **kwargs)
        self.threshold = threshold
        self.precision = keras.metrics.Precision(thresholds=threshold)
        self.recall = keras.metrics.Recall(thresholds=threshold)

    def update_state(self, y_true, y_pred, sample_weight=None):
        y_pred = tf.cast(y_pred >= self.threshold, tf.float32)
        self.precision.update_state(y_true, y_pred, sample_weight)
        self.recall.update_state(y_true, y_pred, sample_weight)

    def result(self):
        p = self.precision.result()
        r = self.recall.result()
        return 2 * ((p * r) / (p + r + 1e-6))

    def reset_state(self):
        self.precision.reset_state()
        self.recall.reset_state()

"""Balanced Playlist Model (Collaborative Filtering)"""

@tf.keras.utils.register_keras_serializable()
class BalancedPlaylistModel(keras.Model):
    """Custom collaborative filtering model architecture"""
    def __init__(self, num_playlists, num_tracks, embedding_size, **kwargs):
        super().__init__(**kwargs)
        self.num_playlists = num_playlists
        self.num_tracks = num_tracks
        self.embedding_size = embedding_size

        # Gated embeddings with residual connection
        self.playlist_gate = layers.Dense(embedding_size, activation='sigmoid')
        self.track_gate = layers.Dense(embedding_size, activation='sigmoid')

        self.playlist_embedding = layers.Embedding(
            num_playlists,
            embedding_size,
            embeddings_regularizer=keras.regularizers.l1_l2(l1=1e-8, l2=1e-8))

        self.track_embedding = layers.Embedding(
            num_tracks,
            embedding_size,
            embeddings_regularizer=keras.regularizers.l1_l2(l1=1e-8, l2=1e-8))

        # Dynamic dropout
        self.dropout = layers.Dropout(0.3, noise_shape=(None, 1))

    def call(self, inputs, training=None):
        pl_idx, tr_idx = inputs[:,0], inputs[:,1]
        pl_emb = self.playlist_embedding(pl_idx)
        tr_emb = self.track_embedding(tr_idx)

        # Gating mechanism
        pl_emb = pl_emb * self.playlist_gate(pl_emb)
        tr_emb = tr_emb * self.track_gate(tr_emb)

        if training:
            pl_emb = self.dropout(pl_emb, training=True)
            tr_emb = self.dropout(tr_emb, training=True)
            self.dropout.rate = min(0.7, self.dropout.rate + 0.02)

        logits = tf.reduce_sum(pl_emb * tr_emb, axis=1) / 0.2
        return ops.sigmoid(logits)

"""# 3. HELPER FUNCTIONS

Load and Preprocess (from the Content based Filtering Model)
"""

def load_and_preprocess(data_path: Path) -> Tuple[pd.DataFrame, np.ndarray, List[str]]:
    """Load and preprocess audio features data"""
    df = pd.read_csv(data_path)
    audio_features = [
        'danceability', 'energy', 'key', 'loudness', 'mode',
        'speechiness', 'acousticness', 'instrumentalness',
        'liveness', 'valence', 'tempo', 'duration_ms', 'time_signature'
    ]

    df = df.dropna(subset=audio_features).copy()
    df = df.reset_index(drop=True)

    scaler = StandardScaler()
    features = scaler.fit_transform(df[audio_features])

    if 'id' not in df.columns:
        df['id'] = df.index.astype(str)

    return df, features, audio_features

"""Load both models and necessary mappings"""

def load_models() -> Dict:
    """Load both models and necessary mappings"""
    # Load Collaborative Filtering model
    collab_model = keras.models.load_model(
        base_path / 'Collaborative_Filtering.keras',
        custom_objects={
            'BinaryF1Score': BinaryF1Score,
            'BalancedPlaylistModel': BalancedPlaylistModel
        }
    )

    # Load mappings
    with open(base_path / 'track_uri_to_index.pkl', 'rb') as f:
        track_uri_to_index = pickle.load(f)
    with open(base_path / 'track_index_to_uri.pkl', 'rb') as f:
        track_index_to_uri = pickle.load(f)
    with open(base_path / 'playlist_id_to_index.pkl', 'rb') as f:
        playlist_id_to_index = pickle.load(f)
    with open(base_path / 'playlist_index_to_id.pkl', 'rb') as f:
        playlist_index_to_id = pickle.load(f)

    # Load Content-Based model
    content_index = AnnoyIndex(13, 'angular')
    content_index.load(str(base_path / 'Content_Filtering.ann'))

    # Load content data
    content_df, _, _ = load_and_preprocess(base_path / 'tracks_features.csv')

    # Load sparse matrix
    sparse_matrix = load_npz(base_path / 'playlist_track_sparse_matrix.npz')

    return {
        'collab_model': collab_model,
        'content_index': content_index,
        'content_df': content_df,
        'track_uri_to_index': track_uri_to_index,
        'track_index_to_uri': track_index_to_uri,
        'playlist_id_to_index': playlist_id_to_index,
        'playlist_index_to_id': playlist_index_to_id,
        'sparse_matrix': sparse_matrix
    }

"""# 4. HYBRID RECOMMENDER CLASS"""

class HybridRecommender:
    def __init__(self, models: Dict):
        """Initialize with loaded models and mappings"""
        self.models = models

        # Create quick access to mappings
        self.track_uri_to_index = models['track_uri_to_index']
        self.track_index_to_uri = models['track_index_to_uri']
        self.playlist_id_to_index = models['playlist_id_to_index']
        self.playlist_index_to_id = models['playlist_index_to_id']
        self.sparse_matrix = models['sparse_matrix']

    def _get_playlist_tracks(self, playlist_id: int) -> List[str]:
        """Get track URIs for a playlist (using sparse matrix)"""
        playlist_idx = self.playlist_id_to_index[playlist_id]
        _, track_indices = self.sparse_matrix[playlist_idx].nonzero()
        return [self.track_index_to_uri[idx] for idx in track_indices
                if idx in self.track_index_to_uri]

    def get_collab_recommendations(self, playlist_id: int, top_k: int = 50) -> List[str]:
        """Get collaborative filtering recommendations (returns URIs)"""
        # Convert playlist ID to index
        playlist_idx = self.playlist_id_to_index[playlist_id]

        # Prepare all possible track indices
        all_track_indices = np.arange(self.models['collab_model'].num_tracks)
        playlist_track_pairs = np.array([[playlist_idx, track_idx]
                                      for track_idx in all_track_indices])

        # Get predictions and return top URIs
        predictions = self.models['collab_model'].predict(playlist_track_pairs, verbose=0).flatten()
        top_indices = np.argsort(predictions)[-top_k:][::-1]
        return [self.track_index_to_uri[idx] for idx in top_indices
                if idx in self.track_index_to_uri]

    def get_content_recommendations(self, track_uris: List[str], top_k: int = 50) -> List[str]:
        """Get content-based recommendations for given track URIs"""
        if not track_uris:
            return []

        # Convert URIs to content dataframe indices
        track_indices = []
        for uri in track_uris:
            matches = self.models['content_df'][self.models['content_df']['id'] == uri]
            if not matches.empty:
                track_indices.append(matches.index[0])

        if not track_indices:
            return []

        # Find similar tracks using Annoy index
        candidate_scores = defaultdict(float)
        playlist_set = set(track_indices)

        for track_idx in track_indices:
            neighbors, dists = self.models['content_index'].get_nns_by_item(
                track_idx, 50, include_distances=True)
            for n_idx, dist in zip(neighbors, dists):
                if n_idx not in playlist_set:
                    cosine_sim = 1 - (dist**2)/2
                    candidate_scores[n_idx] += cosine_sim

        # Get top recommendations and return URIs
        top_indices = sorted(candidate_scores.keys(),
                           key=lambda x: -candidate_scores[x])[:top_k]
        return [self.models['content_df'].iloc[idx]['id']
               for idx in top_indices]

    def _combine_recommendations(self, collab_recs: List[str],
                               content_recs: List[str],
                               collab_weight: float) -> List[str]:
        """Combine recommendations using weighted scoring"""
        # Score each recommendation based on position
        collab_scores = {uri: (len(collab_recs)-i)*collab_weight
                        for i, uri in enumerate(collab_recs)}
        content_scores = {uri: (len(content_recs)-i)*(1-collab_weight)
                         for i, uri in enumerate(content_recs)}

        # Combine scores
        all_uris = set(collab_recs).union(set(content_recs))
        combined_scores = [(uri, collab_scores.get(uri, 0) + content_scores.get(uri, 0))
                         for uri in all_uris]

        # Sort by combined score
        return [uri for uri, score in sorted(combined_scores,
                                            key=lambda x: -x[1])]

    def calculate_dynamic_weight(self, playlist_id: Optional[int] = None,
                               known_tracks: Optional[List[str]] = None) -> float:
        """
        Calculate dynamic weight for hybrid recommendation:
        - For existing playlists: weight based on playlist length
        - For new playlists: weight based on number of seed tracks
        """
        if playlist_id is not None:
            # Existing playlist - use length from sparse matrix
            playlist_idx = self.playlist_id_to_index[playlist_id]
            playlist_length = self.sparse_matrix[playlist_idx].count_nonzero()
        elif known_tracks:
            # New playlist - use number of seed tracks
            playlist_length = len(known_tracks)
        else:
            # No information - default to content-based
            return 0.0

        # Calculate weight based on playlist length
        if playlist_length >= 10:
            return 0.9  # Strong collaborative preference
        elif playlist_length >= 5:
            return 0.7  # Balanced
        elif playlist_length >= 3:
            return 0.4  # Leaning content-based
        else:
            return 0.0  # Pure content-based

    def hybrid_recommend(self, playlist_id: Optional[int] = None,
                        known_tracks: Optional[List[str]] = None,
                        top_k: int = 20,
                        collab_weight: Optional[float] = None) -> List[str]:
        """
        Main hybrid recommendation method:
        - playlist_id: For existing playlists
        - known_tracks: For new playlists or to supplement existing ones
        - top_k: Number of recommendations to return
        - collab_weight: Optional fixed weight (0-1), otherwise calculated dynamically
        """
        # Calculate weight if not provided
        if collab_weight is None:
            collab_weight = self.calculate_dynamic_weight(playlist_id, known_tracks)

        print(f"Using collaborative weight: {collab_weight:.1f}")

        # Get collaborative recommendations if weight > 0
        collab_recs = []
        if playlist_id is not None and collab_weight > 0:
            collab_recs = self.get_collab_recommendations(playlist_id, top_k*3)

        # Get content recommendations
        content_recs = []
        if known_tracks:
            # Use provided seed tracks
            content_recs = self.get_content_recommendations(known_tracks, top_k*3)
        elif collab_recs and collab_weight < 1.0:
            # Use top collaborative results as seeds
            content_recs = self.get_content_recommendations(collab_recs[:5], top_k*3)

        # Combine recommendations
        if collab_recs and content_recs:
            recommendations = self._combine_recommendations(
                collab_recs, content_recs, collab_weight)
        elif collab_recs:
            recommendations = collab_recs
        else:
            recommendations = content_recs

        return recommendations[:top_k]

"""# 5. DEMONSTRATION AND EVALUATION"""

def evaluate_recommender(recommender: HybridRecommender,
                       test_playlists: List[Dict],
                       n_test: int = 20) -> pd.DataFrame:
    """Efficient evaluation with progress tracking"""
    from tqdm.notebook import tqdm
    import time

    results = []
    test_sample = test_playlists[:n_test]  # Only evaluate first n_test playlists

    start_time = time.time()

    for playlist in tqdm(test_sample, desc="Evaluating playlists"):
        try:
            pid = playlist['pid']
            track_uris = [t['track_uri'] for t in playlist['tracks']]

            # Split into seed (80%) and held-out (20%)
            split_idx = int(len(track_uris) * 0.8)
            seed_tracks = track_uris[:split_idx]
            held_out = track_uris[split_idx:]

            # Get recommendations (time this step)
            rec_start = time.time()
            recs = recommender.hybrid_recommend(
                playlist_id=pid,
                known_tracks=seed_tracks,
                top_k=20
            )
            rec_time = time.time() - rec_start

            # Calculate metrics
            hits = len(set(recs) & set(held_out))
            precision = hits / len(recs) if recs else 0
            recall = hits / len(held_out)

            results.append({
                'pid': pid,
                'name': playlist['name'][:30] + "..." if len(playlist['name']) > 30 else playlist['name'],
                'precision': precision,
                'recall': recall,
                'n_tracks': len(track_uris),
                'time_sec': rec_time
            })

        except Exception as e:
            print(f"\nError with playlist {pid}: {str(e)}")
            continue

    total_time = time.time() - start_time
    print(f"\nEvaluation completed on {len(results)} playlists in {total_time:.1f} seconds")
    print(f"Avg time per playlist: {total_time/len(results):.1f}s")

    return pd.DataFrame(results).sort_values('precision', ascending=False)

"""# MAIN EXECUTION"""

if __name__ == "__main__":
    print("Loading models...")
    models = load_models()
    recommender = HybridRecommender(models)

    # Load just the first 50 playlists from test data
    test_path = base_path / 'Playlist Sample test' / 'mpd.slice.1000-1999.json'
    with open(test_path, 'r') as f:
        test_playlists = json.load(f)['playlists'][:50]  # Only load first 50

    # Test CF on training playlists (IDs 0-9999)
    known_pids = [pid for pid in recommender.playlist_id_to_index.keys()][:20]
    known_results = []

    for pid in known_pids:
        tracks = recommender._get_playlist_tracks(pid)
        split = int(0.8 * len(tracks))
        recs = recommender.get_collab_recommendations(pid, 20)
        hits = len(set(recs) & set(tracks[split:]))
        known_results.append(hits / 20)

    print(f"CF Performance on Training Playlists: {np.mean(known_results):.3f}")

    # Evaluate on 20 playlists (or adjust n_test)
    results = evaluate_recommender(recommender, test_playlists, n_test=20)

    # Display results
    print("\nTop performing recommendations:")
    print(results.head())

    print("\nSummary Statistics:")
    print(f"Mean Precision@20: {results['precision'].mean():.3f}")
    print(f"Mean Recall: {results['recall'].mean():.3f}")
    print(f"Recommended Tracks Found: {results['precision'].sum():.1f}/20")