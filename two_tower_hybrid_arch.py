# -*- coding: utf-8 -*-
"""Copy of Two Tower Hybrid Arch.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18DEum0dbddQpYq9mkaEHg01YpQfH9Gn7

# 1. Imports and Setup
"""

import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers, ops
from keras.saving import register_keras_serializable
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import roc_auc_score, f1_score
from typing import List, Dict, Tuple, Optional
import pickle
from pathlib import Path
from collections import defaultdict
from scipy.sparse import load_npz
import json
from tensorflow.python.framework.errors_impl import InvalidArgumentError
from scipy.sparse import load_npz
import time

# Install Annoy if needed
try:
    import annoy
except ImportError:
    print("Installing Annoy...")
    !pip install annoy
    import annoy
from annoy import AnnoyIndex

# Try to import tqdm and install it if necessary
try:
    from tqdm import tqdm
except ImportError:
    print("Installing tqdm...")

# Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

# Path configurations
base_path = Path('/content/drive/MyDrive/Graduation Project/Dev')

"""Load the test dataset"""

def load_test_playlists(json_path: Path) -> List[Dict]:
    """Load test playlists from JSON file"""
    with open(json_path, 'r') as f:
        data = json.load(f)
    return data['playlists']

"""# 2. CUSTOM CLASSES

Binary F1 Score (from the Collaborative Filtering Model)
"""

@tf.keras.utils.register_keras_serializable()
class BinaryF1Score(keras.metrics.Metric):
    """Custom F1 Score metric for model evaluation"""
    def __init__(self, name='f1_score', threshold=0.5, **kwargs):
        super().__init__(name=name, **kwargs)
        self.threshold = threshold
        self.precision = keras.metrics.Precision(thresholds=threshold)
        self.recall = keras.metrics.Recall(thresholds=threshold)

    def update_state(self, y_true, y_pred, sample_weight=None):
        y_pred = tf.cast(y_pred >= self.threshold, tf.float32)
        self.precision.update_state(y_true, y_pred, sample_weight)
        self.recall.update_state(y_true, y_pred, sample_weight)

    def result(self):
        p = self.precision.result()
        r = self.recall.result()
        return 2 * ((p * r) / (p + r + 1e-6))

    def reset_state(self):
        self.precision.reset_state()
        self.recall.reset_state()

"""Balanced Playlist Model (Collaborative Filtering)"""

@tf.keras.utils.register_keras_serializable()
class BalancedPlaylistModel(keras.Model):
    """Custom collaborative filtering model architecture"""
    def __init__(self, num_playlists, num_tracks, embedding_size, **kwargs):
        super().__init__(**kwargs)
        self.num_playlists = num_playlists
        self.num_tracks = num_tracks
        self.embedding_size = embedding_size

        # Gated embeddings with residual connection
        self.track_gate = layers.Dense(embedding_size, activation='sigmoid', name="track_gating")
        self.playlist_gate = layers.Dense(embedding_size, activation='sigmoid', name="playlist_gating")


        self.playlist_embedding = layers.Embedding(
            num_playlists,
            embedding_size,
            name="playlist_embedding",
            embeddings_regularizer=keras.regularizers.l1_l2(l1=1e-8, l2=1e-8))

        self.track_embedding = layers.Embedding(
            num_tracks,
            embedding_size,
            name="track_embedding",
            embeddings_regularizer=keras.regularizers.l1_l2(l1=1e-8, l2=1e-8))

        # Dynamic dropout
        self.dropout = layers.Dropout(0.3, noise_shape=(None, 1))

    def call(self, inputs, training=None):
        pl_idx, tr_idx = inputs[:,0], inputs[:,1]
        pl_emb = self.playlist_embedding(pl_idx)
        tr_emb = self.track_embedding(tr_idx)

        # Gating mechanism
        pl_emb = pl_emb * self.playlist_gate(pl_emb)
        tr_emb = tr_emb * self.track_gate(tr_emb)

        if training:
            pl_emb = self.dropout(pl_emb, training=True)
            tr_emb = self.dropout(tr_emb, training=True)
            self.dropout.rate = min(0.7, self.dropout.rate + 0.02)

        logits = tf.reduce_sum(pl_emb * tr_emb, axis=1) / 0.2
        return ops.sigmoid(logits)

"""# 3. HELPER FUNCTIONS

Playlist Encoder to prepare data for Collaborative Filtering Model
"""

class PlaylistEncoder:
    def encode_new_playlist(self, track_uris: List[str]) -> np.ndarray:
        """Enhanced version for new playlists"""
        track_indices = [
            self.track_uri_to_index[uri]
            for uri in track_uris
            if uri in self.track_uri_to_index
        ]

        if not track_indices:
            return np.zeros((1, self.track_emb_layer.output_dim))

        track_embeddings = self.track_emb_layer(np.array(track_indices))
        gate_weights = self.gate_layer(track_embeddings)
        gated_tracks = track_embeddings * gate_weights

        # Weighted average based on track popularity or other factors
        return np.mean(gated_tracks, axis=0, keepdims=True)

"""Load and Preprocess (from the Content based Filtering Model)"""

def load_and_preprocess(data_path: Path) -> Tuple[pd.DataFrame, np.ndarray, List[str]]:
    """Load and preprocess audio features data"""
    df = pd.read_csv(data_path)
    audio_features = [
        'danceability', 'energy', 'key', 'loudness', 'mode',
        'speechiness', 'acousticness', 'instrumentalness',
        'liveness', 'valence', 'tempo', 'duration_ms', 'time_signature'
    ]

    df = df.dropna(subset=audio_features).copy()
    df = df.reset_index(drop=True)

    scaler = StandardScaler()
    features = scaler.fit_transform(df[audio_features])

    if 'id' not in df.columns:
        df['id'] = df.index.astype(str)

    return df, features, audio_features

"""Load both models and necessary mappings"""

def load_models() -> Dict:
    """Load both models and necessary mappings"""
    # Load Collaborative Filtering model
    collab_model = keras.models.load_model(
        base_path / 'Collaborative_Filtering.keras',
        custom_objects={
            'BinaryF1Score': BinaryF1Score,
            'BalancedPlaylistModel': BalancedPlaylistModel
        }
    )

    # Load mappings
    with open(base_path / 'track_uri_to_index.pkl', 'rb') as f:
        track_uri_to_index = pickle.load(f)
    with open(base_path / 'track_index_to_uri.pkl', 'rb') as f:
        track_index_to_uri = pickle.load(f)
    with open(base_path / 'playlist_id_to_index.pkl', 'rb') as f:
        playlist_id_to_index = pickle.load(f)
    with open(base_path / 'playlist_index_to_id.pkl', 'rb') as f:
        playlist_index_to_id = pickle.load(f)

    # Load Content-Based model
    content_index = AnnoyIndex(13, 'angular')
    content_index.load(str(base_path / 'Content_Filtering.ann'))

    # Load content data
    content_df, _, _ = load_and_preprocess(base_path / 'tracks_features.csv')

    # Load sparse matrix
    sparse_matrix = load_npz(base_path / 'playlist_track_sparse_matrix.npz')

    return {
        'collab_model': collab_model,
        'content_index': content_index,
        'content_df': content_df,
        'track_uri_to_index': track_uri_to_index,
        'track_index_to_uri': track_index_to_uri,
        'playlist_id_to_index': playlist_id_to_index,
        'playlist_index_to_id': playlist_index_to_id,
        'sparse_matrix': sparse_matrix
    }

"""# 4. HYBRID RECOMMENDER CLASS"""

class HybridRecommender:
    def __init__(self, models: Dict, sparse_matrix=None):
        """Initialize with proper sparse matrix handling"""
        self.models = models
        self.track_uri_to_index = models["track_uri_to_index"]
        self.playlist_id_to_index = models["playlist_id_to_index"]
        self.track_index_to_uri = models["track_index_to_uri"]
        self.playlist_index_to_id = models["playlist_index_to_id"]

        # Handle sparse matrix initialization
        self.sparse_matrix = sparse_matrix
        self.has_sparse_matrix = sparse_matrix is not None

        self.encoder = PlaylistEncoder()

    def _get_playlist_tracks(self, playlist_id: int) -> List[str]:
        """Fixed version with better error handling"""
        try:
            playlist_idx = self.playlist_id_to_index[playlist_id]
            _, track_indices = self.sparse_matrix[playlist_idx].nonzero()
            return [self.track_index_to_uri[idx] for idx in track_indices
                if idx in self.track_index_to_uri]
        except KeyError:
            print(f"Playlist ID {playlist_id} not found in index")
            return []
        except IndexError:
            print(f"Playlist index {playlist_idx} out of bounds")
            return []


    def get_collab_recommendations(self, playlist_id: int, top_k: int = 50) -> List[str]:
        """Get collaborative filtering recommendations (returns URIs)"""
        playlist_idx = self.playlist_id_to_index[playlist_id]

        # Get tracks to exclude (already in playlist)
        _, existing_tracks = self.sparse_matrix[playlist_idx].nonzero()
        existing_tracks_set = set(existing_tracks)

        # Predict for all non-existing tracks
        all_track_indices = np.arange(self.models['collab_model'].num_tracks)
        candidate_indices = [idx for idx in all_track_indices
                           if idx not in existing_tracks_set]

        # Batch predictions to avoid memory issues
        batch_size = 10000
        predictions = []
        for i in range(0, len(candidate_indices), batch_size):
            batch = candidate_indices[i:i+batch_size]
            inputs = np.column_stack([
                np.full(len(batch), playlist_idx),
                batch
            ])
            predictions.extend(self.models['collab_model'].predict(inputs, verbose=0).flatten())

        # Get top new recommendations
        top_indices = np.argsort(predictions)[-top_k:][::-1]
        return [self.track_index_to_uri[candidate_indices[i]] for i in top_indices
                if candidate_indices[i] in self.track_index_to_uri]

    def get_content_recommendations(self, track_uris: List[str], top_k: int = 50) -> List[str]:
        """Get content-based recommendations for given track URIs"""
        if not track_uris:
            return []

        # Convert URIs to content dataframe indices
        track_indices = []
        for uri in track_uris:
            matches = self.models['content_df'][self.models['content_df']['id'] == uri]
            if not matches.empty:
                track_indices.append(matches.index[0])

        if not track_indices:
            return []

        # Find similar tracks using Annoy index
        candidate_scores = defaultdict(float)
        playlist_set = set(track_indices)

        for track_idx in track_indices:
            neighbors, dists = self.models['content_index'].get_nns_by_item(
                track_idx, 50, include_distances=True)
            for n_idx, dist in zip(neighbors, dists):
                if n_idx not in playlist_set:
                    cosine_sim = 1 - (dist**2)/2
                    candidate_scores[n_idx] += cosine_sim

        # Get top recommendations
        top_indices = sorted(candidate_scores.keys(),
                           key=lambda x: -candidate_scores[x])[:top_k]
        return [self.models['content_df'].iloc[idx]['id']
               for idx in top_indices]

    def _combine_recommendations(self, collab_recs: List[str],
                               content_recs: List[str],
                               collab_weight: float) -> List[str]:
        """More robust combination with position decay"""
        # Normalize scores using exponential decay
        def get_decay_scores(items, weight):
            return {uri: weight * (0.95 ** i)
                  for i, uri in enumerate(items)}

        collab_scores = get_decay_scores(collab_recs, collab_weight)
        content_scores = get_decay_scores(content_recs, 1 - collab_weight)

        # Combine scores
        all_uris = set(collab_recs) | set(content_recs)
        combined = [(uri,
                    collab_scores.get(uri, 0) + content_scores.get(uri, 0))
                  for uri in all_uris]

        # Sort by score and return
        return [uri for uri, score in sorted(combined,
                                          key=lambda x: -x[1])]

    def _calculate_weight(self, playlist_id: Optional[int],
                        known_tracks: Optional[List[str]]) -> float:
        """Safe weight calculation with sparse matrix check"""
        track_count = 0

        # Case 1: Using playlist ID with sparse matrix available
        if playlist_id is not None and self.has_sparse_matrix:
            if playlist_id in self.playlist_id_to_index:
                playlist_idx = self.playlist_id_to_index[playlist_id]
                try:
                    track_count = self.sparse_matrix[playlist_idx].nnz
                except:
                    track_count = 0

        # Case 2: Using known tracks
        elif known_tracks:
            track_count = len([t for t in known_tracks
                            if t in self.track_uri_to_index])

        # Determine weight based on track count
        if track_count >= 10: return 0.9
        if track_count >= 5: return 0.7
        if track_count >= 3: return 0.4
        return 0.0  # Default to pure content-based for small playlists

    def hybrid_recommend(self, playlist_id: Optional[int] = None,
                        known_tracks: Optional[List[str]] = None,
                        top_k: int = 20,
                        collab_weight: Optional[float] = None) -> List[str]:
        """
        Robust hybrid recommendation with proper sparse matrix handling
        """
        # Input validation
        if not known_tracks and playlist_id is None:
            raise ValueError("Must provide either playlist_id or known_tracks")

        # Convert playlist ID to index if available
        playlist_idx = None
        if playlist_id is not None and playlist_id in self.playlist_id_to_index:
            playlist_idx = self.playlist_id_to_index[playlist_id]

        # Get valid known tracks
        valid_tracks = [t for t in (known_tracks or []) if t in self.track_uri_to_index]

        # Calculate weight
        if collab_weight is None:
            collab_weight = self._calculate_weight(playlist_id, valid_tracks)

        # Generate recommendations
        collab_recs = []
        if playlist_idx is not None and collab_weight > 0 and self.has_sparse_matrix:
            try:
                collab_recs = self.get_collab_recommendations(playlist_id, top_k*3)
            except Exception as e:
                print(f"CF recommendation failed: {str(e)}")

        content_recs = []
        if valid_tracks and collab_weight < 1.0:
            try:
                content_recs = self.get_content_recommendations(valid_tracks, top_k*3)
            except Exception as e:
                print(f"Content-based recommendation failed: {str(e)}")

        # Combine results
        if collab_recs and content_recs:
            return self._combine_recommendations(collab_recs, content_recs, collab_weight)[:top_k]
        return (collab_recs or content_recs)[:top_k]

"""# 5. DEMONSTRATION AND EVALUATION"""

def evaluate_recommender(recommender: HybridRecommender, test_playlists: List[Dict], n_test: int = 20) -> pd.DataFrame:
    results = []

    for playlist in tqdm(test_playlists[:n_test], desc="Evaluating playlists"):
        pid = playlist['pid']
        track_uris = [t['track_uri'] for t in playlist['tracks']]

        # Determine hold-out size
        num_holdout = min(20, max(1, int(len(track_uris) * 0.1)))
        seed_tracks = track_uris[:-num_holdout]
        held_out_tracks = track_uris[-num_holdout:]

        # Get recommendations using only track URIs (ignore playlist ID)
        recs = recommender.hybrid_recommend(known_tracks=seed_tracks, top_k=20)

        # Calculate metrics
        hits = set(recs) & set(held_out_tracks)
        results.append({
            'pid': pid,
            'precision': len(hits) / 20,
            'recall': len(hits) / len(held_out_tracks),
            'n_tracks': len(track_uris),
            'seed_size': len(seed_tracks)
        })

    return pd.DataFrame(results)

# Load models and initialize recommender if not done already
models = load_models()
recommender = HybridRecommender(models)

# Load just the first 50 playlists from test data
test_path = base_path / 'Playlist Sample test' / 'mpd.slice.1000-1999.json'
with open(test_path, 'r') as f:
    test_playlists = json.load(f)['playlists'][:50]

"""# MAIN EXECUTION"""

if __name__ == "__main__":
    print("Loading models...")
    models = load_models()
    sparse_matrix = load_npz('/content/drive/MyDrive/Graduation Project/Dev/playlist_track_sparse_matrix.npz')
    recommender = HybridRecommender(models=models, sparse_matrix=sparse_matrix)

    # Load unseen test playlists
    test_path = base_path / 'Playlist Sample test' / 'mpd.slice.1000-1999.json'
    with open(test_path, 'r') as f:
        test_playlists = json.load(f)['playlists'][:50]  # Adjust slice size if needed

    print("Evaluating hybrid recommender on unseen playlists...")

    try:
        results = evaluate_recommender(recommender, test_playlists)
    except InvalidArgumentError as e:
        print("Caught embedding error: likely due to invalid playlist index.")
        raise e

    # Display results
    print("\nTop performing recommendations:")
    print(results.head())

    print("\nSummary Statistics:")
    print(f"Mean Precision@20: {results['precision'].mean():.3f}")
    print(f"Mean Recall: {results['recall'].mean():.3f}")
    print(f"Recommended Tracks Found: {results['precision'].sum():.1f}/20")

    # Sanity check — raw CF prediction
    print("\nSanity check — raw CF prediction:")
    pid = list(recommender.playlist_id_to_index.keys())[0]
    tracks = recommender._get_playlist_tracks(pid)
    recs = recommender.get_collab_recommendations(pid, top_k=10)
    print(f"CF recommendations for playlist {pid}: {recs[:5]}")
    test_input = np.array([[recommender.playlist_id_to_index[pid], recommender.track_uri_to_index[tracks[0]]]])
    score = recommender.models['collab_model'].predict(test_input, verbose=0)
    print(f"Predicted score for known playlist-track pair: {score[0]:.4f}")

# 1. First ensure sparse matrix is loaded properly
print(f"Sparse matrix available: {recommender.has_sparse_matrix}")

# 2. Test with a known training playlist
if recommender.playlist_id_to_index:
    train_pid = next(iter(recommender.playlist_id_to_index.keys()))
    try:
        train_recs = recommender.hybrid_recommend(playlist_id=train_pid, top_k=10)
        print(f"Recommendations for training playlist: {train_recs[:5]}")
    except Exception as e:
        print(f"Error with training playlist: {str(e)}")

# 3. Test with track-only input
test_tracks = ['spotify:track:6O0BuobS0C8fe9NOPXdyZX', 'spotify:track:4InER8nvZnn3nt56anVVNO']
try:
    test_recs = recommender.hybrid_recommend(known_tracks=test_tracks, top_k=10)
    print(f"Recommendations for track list: {test_recs[:5]}")
except Exception as e:
    print(f"Error with track list: {str(e)}")