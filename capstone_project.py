# -*- coding: utf-8 -*-
"""capstone-project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZdBkbkpHPmYunq85iO9qVLjgeMC7ml9k

# Capstone Project: Collaborative-Based Filtering for Book Recommendations

**Author:** Hosna Qasmei<br>
**Date created:** 09-11-2022<br>
**Last modified:** 09-15-2022<br>
**Description:** Recommending books using a model trained on UCSD Book Graph dataset.

## Introduction

This project demonstrates Collaborative-based filtering using the [UCSD Book Graph dataset](https://sites.google.com/eng.ucsd.edu/ucsdbookgraph/home) to recommend books to users. The UCSD Book Graph goodreads_interactions.csv lists the ratings given by a set of users to a set of books. Our goal is to be able to predict ratings for books a user has not yet read. The books with the highest predicted ratings can then be recommended to the user.

The steps to get the model are as follows:

1. Load datasets from UCSD Book Graph (*Note: Downloaded the files below to my personal Google drive)
  *   goodreads_books.json.gz
  *   goodreads_interactions.csv
  *   book_id_map.csv
  *   user_id_map.csv
2. Cleaned and Filtered data
3. Map user ID to a "user vector" and book ID to a "book vector" via an embedding matrix
4. Compute the dot product between the user vector and book vector, to obtain the a match score between the user and the book (predicted rating).
5. Train the embeddings via gradient descent using all known user-book pairs.
6. Captured the MSE, MASE, Accuracy, F1 score, Precision and Recall of the model

**References:**


*   [UCSD Book Graph](https://sites.google.com/eng.ucsd.edu/ucsdbookgraph/home)
*   [Collaborative Filtering In Recommender Systems](https://www.iteratorshq.com/blog/collaborative-filtering-in-recommender-systems/)
*   [Collaborative Filtering for Movie Recommendations](https://keras.io/examples/structured_data/collaborative_filtering_movielens/)

## Preprocessing

### Mount G-drive
"""

from google.colab import drive
drive.mount('/content/drive')

"""### Import Dependencies"""

!pip install huggingface_hub

!pip install datasets

import gzip
import json

# To store\load the data
import pandas as pd

# To do linear algebra
import numpy as np

# To create plots
import matplotlib.pyplot as plt
import seaborn as sns

# To compute similarities between vectors
from sklearn.metrics import mean_squared_error
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.feature_extraction.text import TfidfVectorizer

# data load progress bars
from tqdm import tqdm

from collections import deque

# To create deep learning models
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras import metrics
from keras.layers import Input, Embedding, Reshape, Dot, Concatenate, Dense, Dropout
from keras.models import Model
import tensorflow as tf
from keras import backend as K

# To stack sparse matrices
from scipy.sparse import vstack
from copy import deepcopy

from scipy.spatial.distance import pdist, squareform

import sklearn
from sklearn.decomposition import TruncatedSVD

# remove unnecessary TF logs
import logging
tf.get_logger().setLevel(logging.ERROR)

# check keras and TF version used
print('TF Version:', tf.__version__)
print('Keras Version:', keras.__version__)

# !pip freeze

"""### Load Data"""

from datasets import load_dataset

book_data = load_dataset("hqasmei/ml-capstone-project-dataset", data_files="book_data.csv")
book_arr = []
for i in book_data['train']:
  book_arr.append(i)
book_df = pd.DataFrame(book_arr)
# len(book_df)
book_df.head()

filtered_data = load_dataset("hqasmei/ml-capstone-project-dataset", data_files="filtered_data.csv")
filtered_arr = []
for i in filtered_data['train']:
  filtered_arr.append(i)
filtered_df = pd.DataFrame(filtered_arr)
# len(filtered_df)
filtered_df.head()

"""## Model-based Approach"""

filtered_df

"""### Encode users and books as interger indices"""

user_ids            = filtered_df["user_id"].unique().tolist()
user2user_encoded   = {x: i for i, x in enumerate(user_ids)}
userencoded2user    = {i: x for i, x in enumerate(user_ids)}
book_ids            = filtered_df["book_id"].unique().tolist()
book2book_encoded   = {x: i for i, x in enumerate(book_ids)}
book_encoded2book   = {i: x for i, x in enumerate(book_ids)}
filtered_df["user"] = filtered_df["user_id"].map(user2user_encoded)
filtered_df["book"] = filtered_df["book_id"].map(book2book_encoded)

num_users         = len(user2user_encoded)
num_books         = len(book_encoded2book)
filtered_df["rating"]      = filtered_df["rating"].values.astype(np.float32)
# min and max ratings will be used to normalize the ratings later
min_rating        = min(filtered_df["rating"])
max_rating        = max(filtered_df["rating"])

print(
    "Number of users: {}, Number of Books: {}, Min rating: {}, Max rating: {}".format(
        num_users, num_books, min_rating, max_rating
    )
)

"""### Prepare training and validation data"""

df = filtered_df.sample(frac=1, random_state=42)
x = df[["user", "book"]].values
y = df["rating"].values
# Assuming training on 90% of the data and testing on 10%.
train_indices = int(0.9 * df.shape[0])
x_train, x_test, y_train, y_test = (
    x[:train_indices],
    x[train_indices:],
    y[:train_indices],
    y[train_indices:],
)

"""### Create the model"""

EMBEDDING_SIZE = 50


class BookRecommender(keras.Model):
    def __init__(self, num_users, num_books, embedding_size, **kwargs):
        super(BookRecommender, self).__init__(**kwargs)
        self.num_users = num_users
        self.num_books = num_books
        self.embedding_size = embedding_size
        self.user_embedding = layers.Embedding(
            num_users,
            embedding_size,
            embeddings_initializer="he_normal",
            embeddings_regularizer=keras.regularizers.l2(1e-6),
        )
        self.user_bias = layers.Embedding(num_users, 1)
        self.book_embedding = layers.Embedding(
            num_books,
            embedding_size,
            embeddings_initializer="he_normal",
            embeddings_regularizer=keras.regularizers.l2(1e-6),
        )
        self.book_bias = layers.Embedding(num_books, 1)

    def call(self, inputs):
        user_vector = self.user_embedding(inputs[:, 0])
        user_bias = self.user_bias(inputs[:, 0])
        book_vector = self.book_embedding(inputs[:, 1])
        book_bias = self.book_bias(inputs[:, 1])
        dot_user_book = tf.tensordot(user_vector, book_vector, 2)
        # Add all the components (including bias)
        x = dot_user_book + user_bias + book_bias
        # The sigmoid activation forces the rating to between 0 and 1
        return tf.nn.sigmoid(x)

def recall_m(y_true, y_pred):
    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))
    recall = true_positives / (possible_positives + K.epsilon())
    return recall

def precision_m(y_true, y_pred):
    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))
    precision = true_positives / (predicted_positives + K.epsilon())
    return precision

def f1_m(y_true, y_pred):
    precision = precision_m(y_true, y_pred)
    recall = recall_m(y_true, y_pred)
    return 2*((precision*recall)/(precision+recall+K.epsilon()))

model = BookRecommender(num_users, num_books, EMBEDDING_SIZE)
model.compile(
    loss='mse',
    optimizer='adam',
    metrics=['accuracy', f1_m, precision_m, recall_m]
)

"""### Train the model based on the data split"""

# Commented out IPython magic to ensure Python compatibility.
# %cd ml-model/
# Clear any logs from previous runs
!rm -rf ./logs/

from tensorflow.keras.callbacks import TensorBoard

tensorboard_callback = TensorBoard(log_dir='logs')

history = model.fit(
    x=x_train,
    y=y_train,
    batch_size=32,
    epochs=5,
    verbose=1,
    validation_data=(x_test, y_test),
    callbacks=[tensorboard_callback]
)

"""### Plot training and validation loss"""

plt.plot(history.history["loss"])
plt.plot(history.history["val_loss"])
plt.title("model loss")
plt.ylabel("loss")
plt.xlabel("epoch")
plt.legend(["train", "test"], loc="upper left")
plt.show()

"""### Show top 10 book recommendations to a user"""

# Let us get a user and see the top recommendations.
user_id            = df.user_id.sample(1).iloc[0]

books_read_by_user = df[df.user_id == user_id]
books_not_read     = book_df[~book_df["book_id"].isin(books_read_by_user.book_id.values)]["book_id"]
books_not_read     = list(set(books_not_read).intersection(set(book2book_encoded.keys())))
books_not_read     = [[book2book_encoded.get(x)] for x in books_not_read]

user_encoder         = user2user_encoded.get(user_id)
user_book_array      = np.hstack(([[user_encoder]] * len(books_not_read), books_not_read))
y_pred               = model.predict(user_book_array).flatten()
top_ratings_indices  = y_pred.argsort()[-10:][::-1]
recommended_book_ids = [book_encoded2book.get(books_not_read[x][0]) for x in top_ratings_indices]

print("Showing recommendations for user: {}".format(user_id))
print("====" * 9)
print("Books with high ratings from user")
print("----" * 8)
top_books_user = (books_read_by_user.sort_values(by="rating", ascending=False).head(5).book_id.values)

book_df_rows = book_df[book_df["book_id"].isin(top_books_user)]
for row in book_df_rows.itertuples():
    print(row.title)

print("----" * 8)
print("Top 10 book recommendations")
print("----" * 8)
recommended_books = book_df[book_df["book_id"].isin(recommended_book_ids)]
for row in recommended_books.itertuples():
    print(row.title)

type(y_pred)

type(y_train)

"""## Log in to Hugging Face CLI"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# ! pip install git+https://github.com/huggingface/huggingface_hub.git@main
# ! sudo apt -qq install git-lfs
# ! git config --global credential.helper store

! huggingface-cli login

# from huggingface_hub import push_to_hub_keras

# push_to_hub_keras(model, 'ml-model')

# %rm -f -r ml-model
# %cd ml-model

! git add *
! git commit -m 'Updated logs'
! git push

"""# Datasets"""

# df.push_to_hub("hqasmei/df")

# book_df.push_to_hub("hqasmei/df")