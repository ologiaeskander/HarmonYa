# -*- coding: utf-8 -*-
"""Keras Collaborative Filtering.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1t1rw-DBNJuIIdqfohY3Tb6A1j4WvhCSn

# 1. Setup and Imports
"""

import os
import pickle
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from pathlib import Path
from sklearn.model_selection import train_test_split
import tensorflow as tf
import keras
from keras import layers
from keras import ops
from scipy.sparse import load_npz, csr_matrix
import random

"""Connect to drive"""

from google.colab import drive
drive.mount('/content/drive')

"""# 2. Load Data and Mappings"""

# Load sparse matrix
sparse_matrix = load_npz('/content/drive/MyDrive/Graduation Project/Dev/playlist_track_sparse_matrix.npz')

# Load mappings
with open('/content/drive/MyDrive/Graduation Project/Dev/track_uri_to_index.pkl', 'rb') as f:
    track_uri_to_index = pickle.load(f)
with open('/content/drive/MyDrive/Graduation Project/Dev/track_index_to_uri.pkl', 'rb') as f:
    track_index_to_uri = pickle.load(f)
with open('/content/drive/MyDrive/Graduation Project/Dev/playlist_id_to_index.pkl', 'rb') as f:
    playlist_id_to_index = pickle.load(f)
with open('/content/drive/MyDrive/Graduation Project/Dev/playlist_index_to_id.pkl', 'rb') as f:
    playlist_index_to_id = pickle.load(f)

"""# 3. Data Preparation"""

def prepare_data(sparse_matrix):
    """Convert sparse matrix to training data with negative sampling"""
    # Binarize the matrix
    sparse_matrix.data = np.ones_like(sparse_matrix.data)
    coo_matrix = sparse_matrix.tocoo()

    # Create positive samples DataFrame
    df = pd.DataFrame({
        "playlist": coo_matrix.row,
        "track": coo_matrix.col,
        "value": coo_matrix.data
    })

    # Generate negative samples
    def generate_negative_samples(df, num_tracks, num_neg_samples=1):
        positive_set = set(zip(df["playlist"], df["track"]))
        negative_samples = []

        for playlist, track in positive_set:
            for _ in range(num_neg_samples):
                neg_track = random.randint(0, num_tracks - 1)
                while (playlist, neg_track) in positive_set:
                    neg_track = random.randint(0, num_tracks - 1)
                negative_samples.append((playlist, neg_track, 0))

        return pd.DataFrame(negative_samples, columns=["playlist", "track", "value"])

    neg_df = generate_negative_samples(df, sparse_matrix.shape[1])
    df_augmented = pd.concat([df, neg_df]).sample(frac=1).reset_index(drop=True)

    return df_augmented

"""# Prepare and split data"""

df_augmented = prepare_data(sparse_matrix)
x = df_augmented[["playlist", "track"]].values
y = df_augmented["value"].values
x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=0.1, random_state=42)

"""# 4. Model Definition"""

@keras.saving.register_keras_serializable()
class BalancedPlaylistModel(keras.Model):
    def __init__(self, num_playlists, num_tracks, embedding_size, **kwargs):
        super().__init__(**kwargs)
        self.num_playlists = num_playlists
        self.num_tracks = num_tracks
        self.embedding_size = embedding_size

        # Layers (same as before)
        self.playlist_gate = layers.Dense(embedding_size, activation='sigmoid')
        self.track_gate = layers.Dense(embedding_size, activation='sigmoid')
        self.playlist_embedding = layers.Embedding(
            num_playlists, embedding_size,
            embeddings_regularizer=keras.regularizers.l1_l2(l1=1e-8, l2=1e-8))
        self.track_embedding = layers.Embedding(
            num_tracks, embedding_size,
            embeddings_regularizer=keras.regularizers.l1_l2(l1=1e-8, l2=1e-8))
        self.dropout = layers.Dropout(0.3, noise_shape=(None, 1))

    def call(self, inputs, training=None):
        pl_idx, tr_idx = inputs[:,0], inputs[:,1]

        # Base embeddings
        pl_emb = self.playlist_embedding(pl_idx)
        tr_emb = self.track_embedding(tr_idx)

        # Gating mechanism
        pl_emb = pl_emb * self.playlist_gate(pl_emb)
        tr_emb = tr_emb * self.track_gate(tr_emb)

        # Progressive dropout during training
        if training:
            pl_emb = self.dropout(pl_emb, training=True)
            tr_emb = self.dropout(tr_emb, training=True)
            self.dropout.rate = min(0.7, self.dropout.rate + 0.02)

        # Similarity with temperature
        logits = tf.reduce_sum(pl_emb * tr_emb, axis=1) / 0.2
        return ops.sigmoid(logits)
    def get_config(self):
        config = super().get_config()
        config.update({
            'num_playlists': self.num_playlists,
            'num_tracks': self.num_tracks,
            'embedding_size': self.embedding_size
        })
        return config

    @classmethod
    def from_config(cls, config):
        if 'config' in config:  # Handle nested config
            return cls(**config['config'])
        return cls(**config)  # Handle direct config

@keras.saving.register_keras_serializable()
class BinaryF1Score(keras.metrics.Metric):
    def __init__(self, name='f1_score', threshold=0.5, **kwargs):
        super().__init__(name=name, **kwargs)
        self.threshold = threshold
        self.precision = keras.metrics.Precision(thresholds=threshold)
        self.recall = keras.metrics.Recall(thresholds=threshold)

    def update_state(self, y_true, y_pred, sample_weight=None):
        y_pred = tf.cast(y_pred >= self.threshold, tf.float32)
        self.precision.update_state(y_true, y_pred, sample_weight)
        self.recall.update_state(y_true, y_pred, sample_weight)

    def result(self):
        p = self.precision.result()
        r = self.recall.result()
        return 2 * ((p * r) / (p + r + 1e-6))

    def reset_state(self):
        self.precision.reset_state()
        self.recall.reset_state()

"""# 5. MODEL TRAINING

Define custom F1 Score
"""

def train_model(x_train, y_train, x_val, y_val, num_playlists, num_tracks):
    model = BalancedPlaylistModel(
        num_playlists=sparse_matrix.shape[0],  # e.g., 10000
        num_tracks=sparse_matrix.shape[1],     # e.g., 170089
        embedding_size=20
    )

    model.compile(
        optimizer=keras.optimizers.Adam(learning_rate=0.001, clipnorm=1.0),
        loss=keras.losses.BinaryCrossentropy(label_smoothing=0.15),
        metrics=[BinaryF1Score(), keras.metrics.AUC()]
    )

    history = model.fit(
        x_train, y_train,
        batch_size=512,
        epochs=10,
        validation_data=(x_val, y_val),
        callbacks=[
            keras.callbacks.EarlyStopping(
                monitor='val_f1_score',
                patience=3,
                mode='max',
                restore_best_weights=True
            ),
            keras.callbacks.ReduceLROnPlateau(
                monitor='val_loss',
                factor=0.6,
                patience=2,
                min_lr=1e-6
            )
        ]
    )

    return model, history

model, history = train_model(x_train, y_train, x_val, y_val,
                            sparse_matrix.shape[0], sparse_matrix.shape[1])

"""Plot results"""

def plot_diagnostic(history):
    plt.figure(figsize=(15, 10))

    # Loss Plot with Overfitting Markers
    plt.subplot(2, 2, 1)
    train_loss = history.history['loss']
    val_loss = history.history['val_loss']

    # Find overfitting point (val loss minimum)
    min_val_loss_epoch = np.argmin(val_loss) + 1
    plt.axvline(min_val_loss_epoch, color='r', linestyle='--', alpha=0.5, label='Overfit Start')

    plt.plot(train_loss, label='Train Loss')
    plt.plot(val_loss, label='Val Loss')
    plt.title('Loss Curves\n(Overfitting starts at red line)')
    plt.ylabel('Loss')
    plt.xlabel('Epoch')
    plt.legend()

    # F1 Score Plot with Confidence Bands
    plt.subplot(2, 2, 2)
    train_f1 = history.history['f1_score']
    val_f1 = history.history['val_f1_score']
    epochs = range(1, len(train_f1) + 1)

    # Calculate and plot the optimal gap (0.1)
    optimal_gap = [t - 0.1 for t in train_f1]
    plt.fill_between(epochs, optimal_gap, val_f1,
                    where=np.array(val_f1) >= np.array(optimal_gap),
                    facecolor='green', alpha=0.2, label='Healthy Gap')
    plt.fill_between(epochs, optimal_gap, val_f1,
                    where=np.array(val_f1) < np.array(optimal_gap),
                    facecolor='red', alpha=0.2, label='Overfitting Gap')

    plt.plot(epochs, train_f1, label='Train F1')
    plt.plot(epochs, val_f1, label='Val F1')
    plt.title('F1 Score with Healthy Zone')
    plt.ylabel('F1 Score')
    plt.xlabel('Epoch')
    plt.legend()

    # AUC Plot with Improvement Tracking
    plt.subplot(2, 2, 3)
    train_auc = history.history['auc']
    val_auc = history.history['val_auc']

    # Mark significant improvements
    improvements = np.where(np.diff(val_auc) > 0.005)[0] + 1
    for imp in improvements:
        plt.axvline(imp, color='g', linestyle=':', alpha=0.3)

    plt.plot(train_auc, label='Train AUC')
    plt.plot(val_auc, label='Val AUC')
    plt.title('AUC (Green = Significant Improvement)')
    plt.ylabel('AUC')
    plt.xlabel('Epoch')
    plt.legend()

    plt.tight_layout()
    plt.show()

plot_diagnostic(history)

"""Evaluate the Model"""

Evaluate = model.evaluate(x_val, y_val)

"""Analyzing predictions"""

# Get predictions for the entire training set
predictions = model.predict(np.concatenate([x_train, x_val])).flatten()
def analyze_predictions(predictions):
    plt.figure(figsize=(12, 5))

    # Enhanced Histogram
    plt.subplot(1, 2, 1)
    counts, bins, _ = plt.hist(predictions, bins=50,
                              color='skyblue', edgecolor='black', alpha=0.7)
    plt.title('Prediction Distribution\n')
    plt.xlabel('Predicted Probability')
    plt.ylabel('Frequency')

    plt.axvline(np.mean(predictions), color='k', linestyle='--', label='Mean')
    plt.legend()

    # Statistical Summary
    print("Min prediction:", np.min(predictions))
    print("Max prediction:", np.max(predictions))
    print("Mean prediction:", np.mean(predictions))
    print("Median prediction:", np.median(predictions))
    print("Standard deviation of predictions:", np.std(predictions))

    # Print percentage of extreme values
    print("\nExtreme Value Analysis:")
    print(f"Predictions < 0.1 with max: {np.sum(predictions < 0.1)/np.sum(predictions)} ")
    print(f"Predictions > 0.9: {np.sum(predictions > 0.9)/np.sum(predictions)} ")
    print(f"0.4 < Predictions < 0.6: {np.sum((predictions >= 0.4) & (predictions <= 0.6))/np.sum(predictions)}")

# Analyze predictions
analyze_predictions(predictions)

"""# 6. RECOMMENDATION SYSTEM"""

class PlaylistRecommender:
    def __init__(self, model, track_index_to_uri, playlist_id_to_index, playlist_index_to_id):
        self.model = model
        self.track_index_to_uri = track_index_to_uri
        self.playlist_id_to_index = playlist_id_to_index
        self.playlist_index_to_id = playlist_index_to_id

    def recommend_for_playlist(self, playlist_id, top_k=10):
        """Get recommendations for a playlist ID (original ID, not index)"""
        # Convert playlist ID to index
        playlist_idx = self.playlist_id_to_index[playlist_id]

        # Prepare all possible track-playlist pairs
        all_track_indices = np.arange(len(self.track_index_to_uri))
        playlist_track_pairs = np.array([[playlist_idx, track_idx] for track_idx in all_track_indices])

        # Get predictions
        predictions = self.model.predict(playlist_track_pairs, verbose=0).flatten()

        # Get top recommendations and convert to URIs
        top_k_indices = np.argsort(predictions)[-top_k:][::-1]
        return [self.track_index_to_uri[idx] for idx in top_k_indices]

    def recommend_for_new_playlist(self, track_uris, top_k=10):
        """Cold-start recommendations for new playlists (using average playlist vector)"""
        # Convert track URIs to indices
        track_indices = [self.track_uri_to_index[uri] for uri in track_uris]

        # Create average playlist vector from input tracks
        playlist_vector = np.mean([self.model.track_embedding(tf.constant([idx]))
                                for idx in track_indices], axis=0)

        # Compare against all tracks
        all_track_vectors = self.model.track_embedding.weights[0]
        similarities = tf.reduce_sum(playlist_vector * all_track_vectors, axis=1) / 0.2
        probabilities = tf.sigmoid(similarities)

        # Get top recommendations (excluding input tracks)
        top_indices = np.argsort(probabilities)[-top_k-len(track_indices):][::-1]
        return [self.track_index_to_uri[idx] for idx in top_indices
                if idx not in track_indices][:top_k]

# Initialize recommender
recommender = PlaylistRecommender(model, track_index_to_uri,
                                playlist_id_to_index, playlist_index_to_id)
recommender.track_uri_to_index = track_uri_to_index

# Example usage:
# For existing playlist (using original ID)
# playlist_id = 12345  # Original playlist ID from your dataset
# recommendations = recommender.recommend_for_playlist(playlist_id)

# For new playlist (cold start)
# track_uris = ['spotify:track:abc123', 'spotify:track:def456']
# recommendations = recommender.recommend_for_new_playlist(track_uris)

"""Added to make the code work when combined with the other model"""

def get_collab_recommendations(playlist_id, model, reverse_track_map, top_k=50):
    """Get raw collaborative filtering recommendations"""
    recommended_indices = recommend_tracks(model, playlist_id=playlist_id, top_k=top_k)
    return [reverse_track_map[idx] for idx in recommended_indices]

def load_collab_model(model_path):
    """Load the saved collaborative model"""
    return keras.models.load_model(model_path, custom_objects={'BinaryF1Score': BinaryF1Score})

"""# 7. SAVE MODEL AND UTILITIES"""

# Save the full model
model.save('/content/drive/MyDrive/Graduation Project/Dev/Collaborative_Filtering.keras')

# Save the recommender class for easy loading
with open('/content/drive/MyDrive/Graduation Project/Dev/playlist_recommender.pkl', 'wb') as f:
    pickle.dump(recommender, f)

#Save the model as .h5
model.save('/content/drive/MyDrive/Collaborative_Filtering.h5')
model.save('/content/drive/MyDrive/Collaborative_Filtering_try.keras', save_format="keras")
keras.saving.save_model(model, '/content/drive/MyDrive/Collaborative_Filtering.keras')